{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.IMPORTING NECESSARY LIBRARIES"
      ],
      "metadata": {
        "id": "pIvTyymaT4qR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oWY4HbKG5s9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.DATASET PREPARATION"
      ],
      "metadata": {
        "id": "SJsLwAH0T_0k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WDrka9mNod-",
        "outputId": "61413080-1583-4d81-c899-19a1a2b43e12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted to: dataset\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/dataset.zip'  # Update if your zip file is located elsewhere\n",
        "extract_to = 'dataset'\n",
        "\n",
        "# Extract the dataset if everything is okay\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to)\n",
        "\n",
        "print(\"Dataset extracted to:\", extract_to)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.DATA AUGMENTATION AND PREPROCESSING AND LOADING PRETRAINED MODELS"
      ],
      "metadata": {
        "id": "GlMeWwdHUnwg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNaGhzrhG8e3",
        "outputId": "fde4e7f3-70c5-4ba6-b92d-57e9c463aa27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1312 images belonging to 1 classes.\n",
            "Found 1312 images belonging to 1 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# Set image size and parameters\n",
        "img_size = (224, 224)\n",
        "batch_size = 32\n",
        "num_classes = 3  # Benign and malignant\n",
        "# Step 1: Data augmentation and preprocessing for training\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "valid_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Load train and test data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    '/content/dataset',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "\n",
        "test_generator = valid_test_datagen.flow_from_directory(\n",
        "    '/content/dataset',\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse'\n",
        ")\n",
        "# Step 2: Load your saved ResNet and VGG models\n",
        "resnet = load_model('/content/resnet_model.h5')\n",
        "vgg = load_model('/content/vgg_19_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.FEATURE EXTRACTION FROM PRETRAINED MODELS\n",
        "\n",
        "5.CREATING A SHARED INPUT LAYER\n",
        "\n",
        "6.COMBING FEATURES AND ADDING CUSTOM CLASSIFICATION LAYERS\n",
        "\n",
        "7.DEFINING AND COMPILING THE MODEL\n",
        "\n",
        "8.TRAINING THE MODEL\n",
        "\n",
        "9.EVALUATING THE MODEL"
      ],
      "metadata": {
        "id": "8qJbWHGtU2fT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq5lRauOHWqq"
      },
      "outputs": [],
      "source": [
        "# Step 3: Remove the top layers (the classification head) to get the features\n",
        "def remove_top(model):\n",
        "    model.layers[-1].activation = None\n",
        "    return models.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "\n",
        "resnet_features = remove_top(resnet)\n",
        "vgg_features = remove_top(vgg)\n",
        "\n",
        "# Step 4: Create a shared input layer\n",
        "input_layer = layers.Input(shape=(224, 224, 3))\n",
        "\n",
        "# Pass the same input through both models\n",
        "resnet_output = resnet_features(input_layer)\n",
        "vgg_output = vgg_features(input_layer)\n",
        "\n",
        "# Step 5: Combine the feature outputs from ResNet and VGG\n",
        "combined = layers.concatenate([resnet_output, vgg_output])\n",
        "\n",
        "# Step 6: Add new fully connected layers for classification\n",
        "x = layers.Dense(512, activation='relu')(combined)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "predictions = layers.Dense(3, activation='softmax')(x)  # Ensure 3 classes\n",
        "\n",
        "# Step 7: Define the new model\n",
        "ensemble_model = models.Model(inputs=input_layer, outputs=predictions)\n",
        "\n",
        "# Step 8: Compile the model\n",
        "ensemble_model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "                       loss='sparse_categorical_crossentropy',  # Ensure this matches your label encoding\n",
        "                       metrics=['accuracy'])\n",
        "\n",
        "# Step 9: Train the model\n",
        "history = ensemble_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=test_generator.samples // batch_size,\n",
        "    epochs=10\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLSIjdy7dONdxMPiCrx1aw"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}